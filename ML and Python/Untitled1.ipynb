{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d51c1c0-8c23-4901-aadb-03f752a3e5d3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'environment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_211759/1998601339.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'environment'"
     ]
    }
   ],
   "source": [
    "#gridworld.py (original)\n",
    "# gridworld.py\n",
    "# ------------\n",
    "# Licensing Information:  You are free to use or extend these projects for\n",
    "# educational purposes provided that (1) you do not distribute or publish\n",
    "# solutions, (2) you retain this notice, and (3) you provide clear\n",
    "# attribution to UC Berkeley, including a link to http://ai.berkeley.edu.\n",
    "# \n",
    "# Attribution Information: The Pacman AI projects were developed at UC Berkeley.\n",
    "# The core projects and autograders were primarily created by John DeNero\n",
    "# (denero@cs.berkeley.edu) and Dan Klein (klein@cs.berkeley.edu).\n",
    "# Student side autograding was added by Brad Miller, Nick Hay, and\n",
    "# Pieter Abbeel (pabbeel@cs.berkeley.edu).\n",
    "\n",
    "\n",
    "import random\n",
    "import sys\n",
    "import mdp\n",
    "import environment\n",
    "import util\n",
    "import optparse\n",
    "\n",
    "class Gridworld(mdp.MarkovDecisionProcess):\n",
    "    \"\"\"\n",
    "      Gridworld\n",
    "    \"\"\"\n",
    "    def __init__(self, grid):\n",
    "        # layout\n",
    "        if type(grid) == type([]): grid = makeGrid(grid)\n",
    "        self.grid = grid\n",
    "\n",
    "        # parameters\n",
    "        self.livingReward = 0.0\n",
    "        self.noise = 0.2\n",
    "\n",
    "    def setLivingReward(self, reward):\n",
    "        \"\"\"\n",
    "        The (negative) reward for exiting \"normal\" states.\n",
    "\n",
    "        Note that in the R+N text, this reward is on entering\n",
    "        a state and therefore is not clearly part of the state's\n",
    "        future rewards.\n",
    "        \"\"\"\n",
    "        self.livingReward = reward\n",
    "\n",
    "    def setNoise(self, noise):\n",
    "        \"\"\"\n",
    "        The probability of moving in an unintended direction.\n",
    "        \"\"\"\n",
    "        self.noise = noise\n",
    "\n",
    "\n",
    "    def getPossibleActions(self, state):\n",
    "        \"\"\"\n",
    "        Returns list of valid actions for 'state'.\n",
    "\n",
    "        Note that you can request moves into walls and\n",
    "        that \"exit\" states transition to the terminal\n",
    "        state under the special action \"done\".\n",
    "        \"\"\"\n",
    "        if state == self.grid.terminalState:\n",
    "            return ()\n",
    "        x,y = state\n",
    "        if type(self.grid[x][y]) == int:\n",
    "            return ('exit',)\n",
    "        return ('north','west','south','east')\n",
    "\n",
    "    def getStates(self):\n",
    "        \"\"\"\n",
    "        Return list of all states.\n",
    "        \"\"\"\n",
    "        # The true terminal state.\n",
    "        states = [self.grid.terminalState]\n",
    "        for x in range(self.grid.width):\n",
    "            for y in range(self.grid.height):\n",
    "                if self.grid[x][y] != '#':\n",
    "                    state = (x,y)\n",
    "                    states.append(state)\n",
    "        return states\n",
    "\n",
    "    def getReward(self, state, action, nextState):\n",
    "        \"\"\"\n",
    "        Get reward for state, action, nextState transition.\n",
    "\n",
    "        Note that the reward depends only on the state being\n",
    "        departed (as in the R+N book examples, which more or\n",
    "        less use this convention).\n",
    "        \"\"\"\n",
    "        if state == self.grid.terminalState:\n",
    "            return 0.0\n",
    "        x, y = state\n",
    "        cell = self.grid[x][y]\n",
    "        if type(cell) == int or type(cell) == float:\n",
    "            return cell\n",
    "        return self.livingReward\n",
    "\n",
    "    def getStartState(self):\n",
    "        for x in range(self.grid.width):\n",
    "            for y in range(self.grid.height):\n",
    "                if self.grid[x][y] == 'S':\n",
    "                    return (x, y)\n",
    "        raise 'Grid has no start state'\n",
    "\n",
    "    def isTerminal(self, state):\n",
    "        \"\"\"\n",
    "        Only the TERMINAL_STATE state is *actually* a terminal state.\n",
    "        The other \"exit\" states are technically non-terminals with\n",
    "        a single action \"exit\" which leads to the true terminal state.\n",
    "        This convention is to make the grids line up with the examples\n",
    "        in the R+N textbook.\n",
    "        \"\"\"\n",
    "        return state == self.grid.terminalState\n",
    "\n",
    "\n",
    "    def getTransitionStatesAndProbs(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns list of (nextState, prob) pairs\n",
    "        representing the states reachable\n",
    "        from 'state' by taking 'action' along\n",
    "        with their transition probabilities.\n",
    "        \"\"\"\n",
    "\n",
    "        if action not in self.getPossibleActions(state):\n",
    "            raise \"Illegal action!\"\n",
    "\n",
    "        if self.isTerminal(state):\n",
    "            return []\n",
    "\n",
    "        x, y = state\n",
    "\n",
    "        if type(self.grid[x][y]) == int or type(self.grid[x][y]) == float:\n",
    "            termState = self.grid.terminalState\n",
    "            return [(termState, 1.0)]\n",
    "\n",
    "        successors = []\n",
    "\n",
    "        northState = (self.__isAllowed(y+1,x) and (x,y+1)) or state\n",
    "        westState = (self.__isAllowed(y,x-1) and (x-1,y)) or state\n",
    "        southState = (self.__isAllowed(y-1,x) and (x,y-1)) or state\n",
    "        eastState = (self.__isAllowed(y,x+1) and (x+1,y)) or state\n",
    "\n",
    "        if action == 'north' or action == 'south':\n",
    "            if action == 'north':\n",
    "                successors.append((northState,1-self.noise))\n",
    "            else:\n",
    "                successors.append((southState,1-self.noise))\n",
    "\n",
    "            massLeft = self.noise\n",
    "            successors.append((westState,massLeft/2.0))\n",
    "            successors.append((eastState,massLeft/2.0))\n",
    "\n",
    "        if action == 'west' or action == 'east':\n",
    "            if action == 'west':\n",
    "                successors.append((westState,1-self.noise))\n",
    "            else:\n",
    "                successors.append((eastState,1-self.noise))\n",
    "\n",
    "            massLeft = self.noise\n",
    "            successors.append((northState,massLeft/2.0))\n",
    "            successors.append((southState,massLeft/2.0))\n",
    "\n",
    "        successors = self.__aggregate(successors)\n",
    "\n",
    "        return successors\n",
    "\n",
    "    def __aggregate(self, statesAndProbs):\n",
    "        counter = util.Counter()\n",
    "        for state, prob in statesAndProbs:\n",
    "            counter[state] += prob\n",
    "        newStatesAndProbs = []\n",
    "        for state, prob in counter.items():\n",
    "            newStatesAndProbs.append((state, prob))\n",
    "        return newStatesAndProbs\n",
    "\n",
    "    def __isAllowed(self, y, x):\n",
    "        if y < 0 or y >= self.grid.height: return False\n",
    "        if x < 0 or x >= self.grid.width: return False\n",
    "        return self.grid[x][y] != '#'\n",
    "\n",
    "class GridworldEnvironment(environment.Environment):\n",
    "\n",
    "    def __init__(self, gridWorld):\n",
    "        self.gridWorld = gridWorld\n",
    "        self.reset()\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        return self.state\n",
    "\n",
    "    def getPossibleActions(self, state):\n",
    "        return self.gridWorld.getPossibleActions(state)\n",
    "\n",
    "    def doAction(self, action):\n",
    "        state = self.getCurrentState()\n",
    "        (nextState, reward) = self.getRandomNextState(state, action)\n",
    "        self.state = nextState\n",
    "        return (nextState, reward)\n",
    "\n",
    "    def getRandomNextState(self, state, action, randObj=None):\n",
    "        rand = -1.0\n",
    "        if randObj is None:\n",
    "            rand = random.random()\n",
    "        else:\n",
    "            rand = randObj.random()\n",
    "        sum = 0.0\n",
    "        successors = self.gridWorld.getTransitionStatesAndProbs(state, action)\n",
    "        for nextState, prob in successors:\n",
    "            sum += prob\n",
    "            if sum > 1.0:\n",
    "                raise 'Total transition probability more than one; sample failure.'\n",
    "            if rand < sum:\n",
    "                reward = self.gridWorld.getReward(state, action, nextState)\n",
    "                return (nextState, reward)\n",
    "        raise 'Total transition probability less than one; sample failure.'\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.gridWorld.getStartState()\n",
    "\n",
    "class Grid:\n",
    "    \"\"\"\n",
    "    A 2-dimensional array of immutables backed by a list of lists.  Data is accessed\n",
    "    via grid[x][y] where (x,y) are cartesian coordinates with x horizontal,\n",
    "    y vertical and the origin (0,0) in the bottom left corner.\n",
    "\n",
    "    The __str__ method constructs an output that is oriented appropriately.\n",
    "    \"\"\"\n",
    "    def __init__(self, width, height, initialValue=' '):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.data = [[initialValue for y in range(height)] for x in range(width)]\n",
    "        self.terminalState = 'TERMINAL_STATE'\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "    def __setitem__(self, key, item):\n",
    "        self.data[key] = item\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if other == None: return False\n",
    "        return self.data == other.data\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.data)\n",
    "\n",
    "    def copy(self):\n",
    "        g = Grid(self.width, self.height)\n",
    "        g.data = [x[:] for x in self.data]\n",
    "        return g\n",
    "\n",
    "    def deepCopy(self):\n",
    "        return self.copy()\n",
    "\n",
    "    def shallowCopy(self):\n",
    "        g = Grid(self.width, self.height)\n",
    "        g.data = self.data\n",
    "        return g\n",
    "\n",
    "    def _getLegacyText(self):\n",
    "        t = [[self.data[x][y] for x in range(self.width)] for y in range(self.height)]\n",
    "        t.reverse()\n",
    "        return t\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self._getLegacyText())\n",
    "\n",
    "def makeGrid(gridString):\n",
    "    width, height = len(gridString[0]), len(gridString)\n",
    "    grid = Grid(width, height)\n",
    "    for ybar, line in enumerate(gridString):\n",
    "        y = height - ybar - 1\n",
    "        for x, el in enumerate(line):\n",
    "            grid[x][y] = el\n",
    "    return grid\n",
    "\n",
    "def getCliffGrid():\n",
    "    grid = [[' ',' ',' ',' ',' '],\n",
    "            ['S',' ',' ',' ',10],\n",
    "            [-100,-100, -100, -100, -100]]\n",
    "    return Gridworld(makeGrid(grid))\n",
    "\n",
    "def getCliffGrid2():\n",
    "    grid = [[' ',' ',' ',' ',' '],\n",
    "            [8,'S',' ',' ',10],\n",
    "            [-100,-100, -100, -100, -100]]\n",
    "    return Gridworld(grid)\n",
    "\n",
    "def getDiscountGrid():\n",
    "    grid = [[' ',' ',' ',' ',' '],\n",
    "            [' ','#',' ',' ',' '],\n",
    "            [' ','#', 1,'#', 10],\n",
    "            ['S',' ',' ',' ',' '],\n",
    "            [-10,-10, -10, -10, -10]]\n",
    "    return Gridworld(grid)\n",
    "\n",
    "def getBridgeGrid():\n",
    "    grid = [[ '#',-100, -100, -100, -100, -100, '#'],\n",
    "            [   1, 'S',  ' ',  ' ',  ' ',  ' ',  10],\n",
    "            [ '#',-100, -100, -100, -100, -100, '#']]\n",
    "    return Gridworld(grid)\n",
    "\n",
    "def getBookGrid():\n",
    "    grid = [[' ',' ',' ',+1],\n",
    "            [' ','#',' ',-1],\n",
    "            ['S',' ',' ',' ']]\n",
    "    return Gridworld(grid)\n",
    "\n",
    "def getMazeGrid():\n",
    "    grid = [[' ',' ',' ',+1],\n",
    "            ['#','#',' ','#'],\n",
    "            [' ','#',' ',' '],\n",
    "            [' ','#','#',' '],\n",
    "            ['S',' ',' ',' ']]\n",
    "    return Gridworld(grid)\n",
    "\n",
    "\n",
    "\n",
    "def getUserAction(state, actionFunction):\n",
    "    \"\"\"\n",
    "    Get an action from the user (rather than the agent).\n",
    "\n",
    "    Used for debugging and lecture demos.\n",
    "    \"\"\"\n",
    "    import graphicsUtils\n",
    "    action = None\n",
    "    while True:\n",
    "        keys = graphicsUtils.wait_for_keys()\n",
    "        if 'Up' in keys: action = 'north'\n",
    "        if 'Down' in keys: action = 'south'\n",
    "        if 'Left' in keys: action = 'west'\n",
    "        if 'Right' in keys: action = 'east'\n",
    "        if 'q' in keys: sys.exit(0)\n",
    "        if action == None: continue\n",
    "        break\n",
    "    actions = actionFunction(state)\n",
    "    if action not in actions:\n",
    "        action = actions[0]\n",
    "    return action\n",
    "\n",
    "def printString(x): print(x)\n",
    "def runEpisode(agent, environment, discount, decision, display, message, pause, episode):\n",
    "    returns = 0\n",
    "    totalDiscount = 1.0\n",
    "    environment.reset()\n",
    "    if 'startEpisode' in dir(agent): agent.startEpisode()\n",
    "    message(\"BEGINNING EPISODE: \"+str(episode)+\"\\n\")\n",
    "    while True:\n",
    "\n",
    "        # DISPLAY CURRENT STATE\n",
    "        state = environment.getCurrentState()\n",
    "        display(state)\n",
    "        pause()\n",
    "\n",
    "        # END IF IN A TERMINAL STATE\n",
    "        actions = environment.getPossibleActions(state)\n",
    "        if len(actions) == 0:\n",
    "            message(\"EPISODE \"+str(episode)+\" COMPLETE: RETURN WAS \"+str(returns)+\"\\n\")\n",
    "            return returns\n",
    "\n",
    "        # GET ACTION (USUALLY FROM AGENT)\n",
    "        action = decision(state)\n",
    "        if action == None:\n",
    "            raise 'Error: Agent returned None action'\n",
    "\n",
    "        # EXECUTE ACTION\n",
    "        nextState, reward = environment.doAction(action)\n",
    "        message(\"Started in state: \"+str(state)+\n",
    "                \"\\nTook action: \"+str(action)+\n",
    "                \"\\nEnded in state: \"+str(nextState)+\n",
    "                \"\\nGot reward: \"+str(reward)+\"\\n\")\n",
    "        # UPDATE LEARNER\n",
    "        if 'observeTransition' in dir(agent):\n",
    "            agent.observeTransition(state, action, nextState, reward)\n",
    "\n",
    "        returns += reward * totalDiscount\n",
    "        totalDiscount *= discount\n",
    "\n",
    "    if 'stopEpisode' in dir(agent):\n",
    "        agent.stopEpisode()\n",
    "\n",
    "def parseOptions():\n",
    "    optParser = optparse.OptionParser()\n",
    "    optParser.add_option('-d', '--discount',action='store',\n",
    "                         type='float',dest='discount',default=0.9,\n",
    "                         help='Discount on future (default %default)')\n",
    "    optParser.add_option('-r', '--livingReward',action='store',\n",
    "                         type='float',dest='livingReward',default=0.0,\n",
    "                         metavar=\"R\", help='Reward for living for a time step (default %default)')\n",
    "    optParser.add_option('-n', '--noise',action='store',\n",
    "                         type='float',dest='noise',default=0.2,\n",
    "                         metavar=\"P\", help='How often action results in ' +\n",
    "                         'unintended direction (default %default)' )\n",
    "    optParser.add_option('-e', '--epsilon',action='store',\n",
    "                         type='float',dest='epsilon',default=0.3,\n",
    "                         metavar=\"E\", help='Chance of taking a random action in q-learning (default %default)')\n",
    "    optParser.add_option('-l', '--learningRate',action='store',\n",
    "                         type='float',dest='learningRate',default=0.5,\n",
    "                         metavar=\"P\", help='TD learning rate (default %default)' )\n",
    "    optParser.add_option('-i', '--iterations',action='store',\n",
    "                         type='int',dest='iters',default=10,\n",
    "                         metavar=\"K\", help='Number of rounds of value iteration (default %default)')\n",
    "    optParser.add_option('-k', '--episodes',action='store',\n",
    "                         type='int',dest='episodes',default=1,\n",
    "                         metavar=\"K\", help='Number of epsiodes of the MDP to run (default %default)')\n",
    "    optParser.add_option('-g', '--grid',action='store',\n",
    "                         metavar=\"G\", type='string',dest='grid',default=\"BookGrid\",\n",
    "                         help='Grid to use (case sensitive; options are BookGrid, BridgeGrid, CliffGrid, MazeGrid, default %default)' )\n",
    "    optParser.add_option('-w', '--windowSize', metavar=\"X\", type='int',dest='gridSize',default=150,\n",
    "                         help='Request a window width of X pixels *per grid cell* (default %default)')\n",
    "    optParser.add_option('-a', '--agent',action='store', metavar=\"A\",\n",
    "                         type='string',dest='agent',default=\"random\",\n",
    "                         help='Agent type (options are \\'random\\', \\'value\\' and \\'q\\', default %default)')\n",
    "    optParser.add_option('-t', '--text',action='store_true',\n",
    "                         dest='textDisplay',default=False,\n",
    "                         help='Use text-only ASCII display')\n",
    "    optParser.add_option('-p', '--pause',action='store_true',\n",
    "                         dest='pause',default=False,\n",
    "                         help='Pause GUI after each time step when running the MDP')\n",
    "    optParser.add_option('-q', '--quiet',action='store_true',\n",
    "                         dest='quiet',default=False,\n",
    "                         help='Skip display of any learning episodes')\n",
    "    optParser.add_option('-s', '--speed',action='store', metavar=\"S\", type=float,\n",
    "                         dest='speed',default=1.0,\n",
    "                         help='Speed of animation, S > 1.0 is faster, 0.0 < S < 1.0 is slower (default %default)')\n",
    "    optParser.add_option('-m', '--manual',action='store_true',\n",
    "                         dest='manual',default=False,\n",
    "                         help='Manually control agent')\n",
    "    optParser.add_option('-v', '--valueSteps',action='store_true' ,default=False,\n",
    "                         help='Display each step of value iteration')\n",
    "\n",
    "    opts, args = optParser.parse_args()\n",
    "\n",
    "    if opts.manual and opts.agent != 'q':\n",
    "        print('## Disabling Agents in Manual Mode (-m) ##')\n",
    "        opts.agent = None\n",
    "\n",
    "    # MANAGE CONFLICTS\n",
    "    if opts.textDisplay or opts.quiet:\n",
    "    # if opts.quiet:\n",
    "        opts.pause = False\n",
    "        # opts.manual = False\n",
    "\n",
    "    if opts.manual:\n",
    "        opts.pause = True\n",
    "\n",
    "    return opts\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    opts = parseOptions()\n",
    "\n",
    "    ###########################\n",
    "    # GET THE GRIDWORLD\n",
    "    ###########################\n",
    "\n",
    "    import gridworld\n",
    "    mdpFunction = getattr(gridworld, \"get\"+opts.grid)\n",
    "    mdp = mdpFunction()\n",
    "    mdp.setLivingReward(opts.livingReward)\n",
    "    mdp.setNoise(opts.noise)\n",
    "    env = gridworld.GridworldEnvironment(mdp)\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # GET THE DISPLAY ADAPTER\n",
    "    ###########################\n",
    "\n",
    "    import textGridworldDisplay\n",
    "    display = textGridworldDisplay.TextGridworldDisplay(mdp)\n",
    "    if not opts.textDisplay:\n",
    "        import graphicsGridworldDisplay\n",
    "        display = graphicsGridworldDisplay.GraphicsGridworldDisplay(mdp, opts.gridSize, opts.speed)\n",
    "    try:\n",
    "        display.start()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(0)\n",
    "\n",
    "    ###########################\n",
    "    # GET THE AGENT\n",
    "    ###########################\n",
    "\n",
    "    import valueIterationAgents, qlearningAgents\n",
    "    a = None\n",
    "    if opts.agent == 'value':\n",
    "        a = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, opts.iters)\n",
    "    elif opts.agent == 'q':\n",
    "        #env.getPossibleActions, opts.discount, opts.learningRate, opts.epsilon\n",
    "        #simulationFn = lambda agent, state: simulation.GridworldSimulation(agent,state,mdp)\n",
    "        gridWorldEnv = GridworldEnvironment(mdp)\n",
    "        actionFn = lambda state: mdp.getPossibleActions(state)\n",
    "        qLearnOpts = {'gamma': opts.discount,\n",
    "                      'alpha': opts.learningRate,\n",
    "                      'epsilon': opts.epsilon,\n",
    "                      'actionFn': actionFn}\n",
    "        a = qlearningAgents.QLearningAgent(**qLearnOpts)\n",
    "    elif opts.agent == 'random':\n",
    "        # # No reason to use the random agent without episodes\n",
    "        if opts.episodes == 0:\n",
    "            opts.episodes = 10\n",
    "        class RandomAgent:\n",
    "            def getAction(self, state):\n",
    "                return random.choice(mdp.getPossibleActions(state))\n",
    "            def getValue(self, state):\n",
    "                return 0.0\n",
    "            def getQValue(self, state, action):\n",
    "                return 0.0\n",
    "            def getPolicy(self, state):\n",
    "                \"NOTE: 'random' is a special policy value; don't use it in your code.\"\n",
    "                return 'random'\n",
    "            def update(self, state, action, nextState, reward):\n",
    "                pass\n",
    "        a = RandomAgent()\n",
    "    else:\n",
    "        if not opts.manual: raise 'Unknown agent type: '+opts.agent\n",
    "\n",
    "\n",
    "    ###########################\n",
    "    # RUN EPISODES\n",
    "    ###########################\n",
    "    # DISPLAY Q/V VALUES BEFORE SIMULATION OF EPISODES\n",
    "    try:\n",
    "        if not opts.manual and opts.agent == 'value':\n",
    "            if opts.valueSteps:\n",
    "                for i in range(opts.iters):\n",
    "                    tempAgent = valueIterationAgents.ValueIterationAgent(mdp, opts.discount, i)\n",
    "                    display.displayValues(tempAgent, message = \"VALUES AFTER \"+str(i)+\" ITERATIONS\")\n",
    "                    display.pause()\n",
    "\n",
    "            display.displayValues(a, message = \"VALUES AFTER \"+str(opts.iters)+\" ITERATIONS\")\n",
    "            display.pause()\n",
    "            display.displayQValues(a, message = \"Q-VALUES AFTER \"+str(opts.iters)+\" ITERATIONS\")\n",
    "            display.pause()\n",
    "    except KeyboardInterrupt:\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "\n",
    "    # FIGURE OUT WHAT TO DISPLAY EACH TIME STEP (IF ANYTHING)\n",
    "    displayCallback = lambda x: None\n",
    "    if not opts.quiet:\n",
    "        if opts.manual and opts.agent == None:\n",
    "            displayCallback = lambda state: display.displayNullValues(state)\n",
    "        else:\n",
    "            if opts.agent == 'random': displayCallback = lambda state: display.displayValues(a, state, \"CURRENT VALUES\")\n",
    "            if opts.agent == 'value': displayCallback = lambda state: display.displayValues(a, state, \"CURRENT VALUES\")\n",
    "            if opts.agent == 'q': displayCallback = lambda state: display.displayQValues(a, state, \"CURRENT Q-VALUES\")\n",
    "\n",
    "    messageCallback = lambda x: printString(x)\n",
    "    if opts.quiet:\n",
    "        messageCallback = lambda x: None\n",
    "\n",
    "    # FIGURE OUT WHETHER TO WAIT FOR A KEY PRESS AFTER EACH TIME STEP\n",
    "    pauseCallback = lambda : None\n",
    "    if opts.pause:\n",
    "        pauseCallback = lambda : display.pause()\n",
    "\n",
    "    # FIGURE OUT WHETHER THE USER WANTS MANUAL CONTROL (FOR DEBUGGING AND DEMOS)\n",
    "    if opts.manual:\n",
    "        decisionCallback = lambda state : getUserAction(state, mdp.getPossibleActions)\n",
    "    else:\n",
    "        decisionCallback = a.getAction\n",
    "\n",
    "    # RUN EPISODES\n",
    "    if opts.episodes > 0:\n",
    "        print\n",
    "        print(\"RUNNING\", opts.episodes, \"EPISODES\")\n",
    "        print\n",
    "    returns = 0\n",
    "    for episode in range(1, opts.episodes+1):\n",
    "        returns += runEpisode(a, env, opts.discount, decisionCallback, displayCallback, messageCallback, pauseCallback, episode)\n",
    "    if opts.episodes > 0:\n",
    "        print\n",
    "        print(\"AVERAGE RETURNS FROM START STATE: \"+str((returns+0.0) / opts.episodes))\n",
    "        print\n",
    "        print\n",
    "\n",
    "    # DISPLAY POST-LEARNING VALUES / Q-VALUES\n",
    "    if opts.agent == 'q' and not opts.manual:\n",
    "        try:\n",
    "            display.displayQValues(a, message = \"Q-VALUES AFTER \"+str(opts.episodes)+\" EPISODES\")\n",
    "            display.pause()\n",
    "            display.displayValues(a, message = \"VALUES AFTER \"+str(opts.episodes)+\" EPISODES\")\n",
    "            display.pause()\n",
    "        except KeyboardInterrupt:\n",
    "            sys.exit(0)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f14f56-f18d-491b-9a90-2e42c34839c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
