# -*- coding: utf-8 -*-

##Master Link - https://colab.research.google.com/drive/1asSxMbBcbrxXA8_jT8HMTb4oturJVVSQ?usp=sharing

"""AI in Business & AutoML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1asSxMbBcbrxXA8_jT8HMTb4oturJVVSQ

# TASK #1: UNDERSTAND THE PROBLEM STATEMENT AND BUSINESS CASE

![alt text](https://drive.google.com/uc?id=1DdfcCuoUZL7MuLgpCXnvCla7ylP6O7x6)

![alt text](https://drive.google.com/uc?id=19PlsX8wH0uDQs_UtEThfitkTTvogXiVW)

![alt text](https://drive.google.com/uc?id=1yAGwj4jwm3o-JVAh3NNcKTHg5TU1D6mn)

![alt text](https://drive.google.com/uc?id=1lrb-KWEaQH91wfRAq9hrdj38gd-sMLhR)

Any publications based on this dataset should acknowledge the following:

Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

The original dataset can be found here at the UCI Machine Learning Repository.

# TASK #2: IMPORT LIBRARIES AND DATASETS
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# You will need to mount your drive using the following commands:
# For more information regarding mounting, please check this out: https://stackoverflow.com/questions/46986398/import-data-into-google-colaboratory

from google.colab import drive
drive.mount('/content/drive')

# You have to include the full link to the csv file containing your dataset
creditcard_df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Modern AI Portfolio Builder/Business AI/UCI_Credit_Card.csv')

creditcard_df

creditcard_df.info()
# 24 features in total, each contains 30000 data points

creditcard_df.describe()
# the mean for LIMIT_BAL = 1500, min =1, and max = 30000
# the mean for AGE = 25 years old, min = 21, and max = 79
# PAY_AMT average is around 5k

"""# TASK #3: VISUALIZE DATASET"""

# Let's see if we have any missing data, luckily we don't!
sns.heatmap(creditcard_df.isnull(), yticklabels = False, cbar = False, cmap="Blues")

creditcard_df.hist(bins = 30, figsize = (20,20), color = 'r')

# Let's drop the ID column
creditcard_df.drop(['ID'], axis=1, inplace=True)

creditcard_df

# Let's see how many customers could potentially default on their credit card payment! 
cc_default_df        = creditcard_df[creditcard_df['default.payment.next.month'] == 1]
cc_nodefault_df      = creditcard_df[creditcard_df['default.payment.next.month'] == 0]

# Count the number of employees who stayed and left
# It seems that we are dealing with an imbalanced dataset 

print("Total =", len(creditcard_df))

print("Number of customers who defaulted on their credit card payments =", len(cc_default_df))
print("Percentage of customers who defaulted on their credit card payments =", 1.*len(cc_default_df)/len(creditcard_df)*100.0, "%")
 
print("Number of customers who did not default on their credit card payments (paid their balance)=", len(cc_nodefault_df))
print("Percentage of customers who did not default on their credit card payments (paid their balance)=", 1.*len(cc_nodefault_df)/len(creditcard_df)*100.0, "%")

# Let's compare the mean and std of the customers who stayed and left 
cc_default_df.describe()

# Let's compare the mean and std of the customers who stayed and left 
cc_nodefault_df.describe()

correlations = creditcard_df.corr()
f, ax = plt.subplots(figsize = (20, 20))
sns.heatmap(correlations, annot = True)

plt.figure(figsize=[25, 12])
sns.countplot(x = 'AGE', hue = 'default.payment.next.month', data = creditcard_df)

plt.figure(figsize=[20,20])
plt.subplot(311)
sns.countplot(x = 'EDUCATION', hue = 'default.payment.next.month', data = creditcard_df)
plt.subplot(312)
sns.countplot(x = 'SEX', hue = 'default.payment.next.month', data = creditcard_df)
plt.subplot(313)
sns.countplot(x = 'MARRIAGE', hue = 'default.payment.next.month', data = creditcard_df)

# KDE (Kernel Density Estimate) is used for visualizing the Probability Density of a continuous variable. 
# KDE describes the probability density at different values in a continuous variable. 

plt.figure(figsize=(12,7))

sns.distplot(cc_nodefault_df['LIMIT_BAL'], bins = 250, color = 'r')
sns.distplot(cc_default_df['LIMIT_BAL'], bins = 250, color = 'b')

plt.xlabel('Amount of bill statement in September, 2005 (NT dollar)')
#plt.xlim(0, 200000)

# KDE (Kernel Density Estimate) is used for visualizing the Probability Density of a continuous variable. 
# KDE describes the probability density at different values in a continuous variable. 

plt.figure(figsize=(12,7))

sns.kdeplot(cc_nodefault_df['BILL_AMT1'], label = 'Customers who did not default (paid balance)', shade = True, color = 'r')
sns.kdeplot(cc_default_df['BILL_AMT1'], label = 'Customers who defaulted (did not pay balance)', shade = True, color = 'b')

plt.xlabel('Amount of bill statement in September, 2005 (NT dollar)')
#plt.xlim(0, 200000)

# KDE (Kernel Density Estimate) is used for visualizing the Probability Density of a continuous variable. 
# KDE describes the probability density at different values in a continuous variable. 

plt.figure(figsize=(12,7))

sns.kdeplot(cc_nodefault_df['PAY_AMT1'], label = 'Customers who did not default (paid balance)', shade = True, color = 'r')
sns.kdeplot(cc_default_df['PAY_AMT1'], label = 'Customers who defaulted (did not pay balance)', shade = True, color = 'b')

plt.xlabel('PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)')
plt.xlim(0, 200000)

# Let's see the impact of sex on the limit balance 

plt.figure(figsize=[10,20])
plt.subplot(211)
sns.boxplot(x = 'SEX', y = 'LIMIT_BAL', data = creditcard_df, showfliers = False)
plt.subplot(212)
sns.boxplot(x = 'SEX', y = 'LIMIT_BAL', data = creditcard_df)

plt.figure(figsize=[10,20])
plt.subplot(211)
sns.boxplot(x = 'MARRIAGE', y = 'LIMIT_BAL', data = creditcard_df, showfliers = False)
plt.subplot(212)
sns.boxplot(x = 'MARRIAGE', y = 'LIMIT_BAL', data = creditcard_df)

"""# TASK #4: CREATE TESTING AND TRAINING DATASET & PERFORM DATA CLEANING"""

creditcard_df

X_cat = creditcard_df[['SEX', 'EDUCATION', 'MARRIAGE']]
X_cat

from sklearn.preprocessing import OneHotEncoder
onehotencoder = OneHotEncoder()
X_cat = onehotencoder.fit_transform(X_cat).toarray()

X_cat.shape

X_cat = pd.DataFrame(X_cat)

X_cat

# note that we dropped the target 'default.payment.next.month'
X_numerical = creditcard_df[['LIMIT_BAL', 'AGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6', 
                'BILL_AMT1','BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6',
                'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']]
X_numerical

X_all = pd.concat([X_cat, X_numerical], axis = 1)
X_all

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X = scaler.fit_transform(X_all)

y = creditcard_df['default.payment.next.month']
y

"""# TASK #5: UNDERSTAND THE THEORY AND INTUITION BEHIND XGBOOST ALGORITHM

![alt text](https://drive.google.com/uc?id=1jFH01bj3HFgojFZoAf4lHPRo5t0Mmktx)

![alt text](https://drive.google.com/uc?id=16K75Is5UOP-YQqneIR0yUpiR-Q7K5hfJ)

![alt text](https://drive.google.com/uc?id=1JAfjOhzUMsfJOvcOzPvMMYQN56JHXW3R)

![alt text](https://drive.google.com/uc?id=1dn61Jz_zGpKv7VTLQvLbp_0S1XwP9X6W)

![alt text](https://drive.google.com/uc?id=1qW-p617bJnRWzh-fWCQfXWyHF4F6vFzD)

![alt text](https://drive.google.com/uc?id=1vywPBzlXqqrcg1K_rqEfeAjZAXjxXCCk)

![alt text](https://drive.google.com/uc?id=1ZRG6Uo7r6W9DBFO5tDh90-xA57ajnnnn)

![alt text](https://drive.google.com/uc?id=19t3DHRXvoz6o7-eG90L-4jHe0cfBdH_x)

![alt text](https://drive.google.com/uc?id=15xcYP8mOOEgUnR0rIXSovoIqUY3ySJRb)

![alt text](https://drive.google.com/uc?id=1Q3UzxrlwbH9jZkl6tEDO2L5AOanS-kqD)

![alt text](https://drive.google.com/uc?id=1rDFPOeoX8pM60ZxnIg-K66IjbqRHHnPG)

# TASK #6: UNDERSTAND XGBOOST ALGORITHM KEY STEPS

![alt text](https://drive.google.com/uc?id=13wDmGpYDA0c-lewGJWXpjVJcPav381Lh)

![alt text](https://drive.google.com/uc?id=1Pxbku6A5FvJsvCsVl6sKk6N3PHg5_IRI)

![alt text](https://drive.google.com/uc?id=1Q6-oFxjZEAU4qUFzPXj3hKXQMWQlPx99)

![alt text](https://drive.google.com/uc?id=1djdQCC99-bZBRB7txj_jT68a8-IHixl6)

![alt text](https://drive.google.com/uc?id=1cStZqzA9ez-EfvSoM8S_1xEMtYBjRMOB)

![alt text](https://drive.google.com/uc?id=1IF2iJ_uquTRJ89lA5PIEX0WfF42ONIwt)

![alt text](https://drive.google.com/uc?id=1FoknM1AqDlDa4xdLyshs0KNE4kA1nCLY)

![alt text](https://drive.google.com/uc?id=1ZJ5vqYAur2p9G6bZkO9XjPbeSME1k3Wo)

![alt text](https://drive.google.com/uc?id=1XMBkju1Ej6RXm-lEvVKwuiMklnUv6WWT)

![alt text](https://drive.google.com/uc?id=1jINWBW2OT1BPLK6As0LyYESdcGcnbrYM)

![alt text](https://drive.google.com/uc?id=1jgF3Q2FUTbNpAC-d5EDXOVAa7xnI68mK)

# TASK #7: TRAIN AND EVALUATE AN XGBOOST CLASSIFIER (LOCALLY)
"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)

X_train.shape

X_test.shape

!pip install xgboost

# Train an XGBoost regressor model 

import xgboost as xgb


model = xgb.XGBClassifier(objective ='reg:squarederror', learning_rate = 0.1, max_depth = 5, n_estimators = 100)

model.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = model.predict(X_test)

y_pred

from sklearn.metrics import confusion_matrix, classification_report

print("Accuracy {} %".format( 100 * accuracy_score(y_pred, y_test)))

# Testing Set Performance
cm = confusion_matrix(y_pred, y_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_pred))

"""# TASK #8: OPTIMIZE XGBOOST HYPERPARAMETERS BY PERFORMING GRID SEARCH"""

param_grid = {
        'gamma': [0.5, 1, 5],   # regularization parameter 
        'subsample': [0.6, 0.8, 1.0], # % of rows taken to build each tree
        'colsample_bytree': [0.6, 0.8, 1.0], # number of columns used by each tree
        'max_depth': [3, 4, 5] # depth of each tree
        }

from xgboost import XGBClassifier

xgb_model = XGBClassifier(learning_rate=0.01, n_estimators=100, objective='binary:logistic')
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(xgb_model, param_grid, refit = True, verbose = 4)
grid.fit(X_train, y_train)

y_predict_optim = grid.predict(X_test)

y_predict_optim

# Testing Set Performance
cm = confusion_matrix(y_predict_optim, y_test)
sns.heatmap(cm, annot=True)

print(classification_report(y_test, y_predict_optim))

"""# TASK #9: XG-BOOST ALGORITHM IN AWS SAGEMAKER

![alt text](https://drive.google.com/uc?id=1ThDuQ1Oq_q57ETsDeGzgtAmDaEDWcKUp)

![alt text](https://drive.google.com/uc?id=1Nr2R7kqpUV5dKnV_FynvPQKTvZMPbxJ9)

![alt text](https://drive.google.com/uc?id=17Lojgwxrh_az4MtItAaB6QgZvf_bJ3Ql)

![alt text](https://drive.google.com/uc?id=1C65sLJA1WmzKWVnckz3RsmRdxS24q_D-)

![alt text](https://drive.google.com/uc?id=17ebCLeT-d-YFITX_SW-x2v-xEKvopFav)

# TASK #10: TRAIN XG-BOOST USING SAGEMAKER
"""

# Convert the array into dataframe in a way that target variable is set as the first column and followed by feature columns
# This is because sagemaker built-in algorithm expects the data in this format.

train_data = pd.DataFrame({'Target': y_train[:,0]})
for i in range(X_train.shape[1]):
    train_data[i] = X_train[:,i]

train_data.head()

val_data = pd.DataFrame({'Target':y_val[:,0]})
for i in range(X_val.shape[1]):
    val_data[i] = X_val[:,i]

val_data.head()

val_data.shape

# save train_data and validation_data as csv files.

train_data.to_csv('train.csv', header = False, index = False)
val_data.to_csv('validation.csv', header = False, index = False)

# Boto3 is the Amazon Web Services (AWS) Software Development Kit (SDK) for Python
# Boto3 allows Python developer to write software that makes use of services like Amazon S3 and Amazon EC2

import sagemaker
import boto3

# Create a sagemaker session
sagemaker_session = sagemaker.Session()

#S 3 bucket and prefix that we want to use
# default_bucket - creates a Amazon S3 bucket to be used in this session
bucket = 'sagemaker-practical-3'
prefix = 'XGBoost-Regressor'
key = 'XGBoost-Regressor'
#Roles give learning and hosting access to the data
#This is specified while opening the sagemakers instance in "Create an IAM role"
role = sagemaker.get_execution_role()

print(role)

# read the data from csv file and then upload the data to s3 bucket
import os
with open('train.csv','rb') as f:
    # The following code uploads the data into S3 bucket to be accessed later for training
    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(f)

# Let's print out the training data location in s3
s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)
print('uploaded training data location: {}'.format(s3_train_data))

# read the data from csv file and then upload the data to s3 bucket

with open('validation.csv','rb') as f:
    # The following code uploads the data into S3 bucket to be accessed later for training

    boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation', key)).upload_fileobj(f)
# Let's print out the validation data location in s3
s3_validation_data = 's3://{}/{}/validation/{}'.format(bucket, prefix, key)
print('uploaded validation data location: {}'.format(s3_validation_data))

# creates output placeholder in S3 bucket to store the output

output_location = 's3://{}/{}/output'.format(bucket, prefix)
print('training artifacts will be uploaded to: {}'.format(output_location))

# This code is used to get the training container of sagemaker built-in algorithms
# all we have to do is to specify the name of the algorithm, that we want to use

# Let's obtain a reference to the XGBoost container image
# Note that all regression models are named estimators
# You don't have to specify (hardcode) the region, get_image_uri will get the current region name using boto3.Session

from sagemaker.amazon.amazon_estimator import get_image_uri

container = get_image_uri(boto3.Session().region_name, 'xgboost','0.90-2') # Latest version of XGboost

# Specify the type of instance that we would like to use for training 
# output path and sagemaker session into the Estimator. 
# We can also specify how many instances we would like to use for training

# Recall that XGBoost works by combining an ensemble of weak models to generate accurate/robust results. 
# The weak models are randomized to avoid overfitting

# num_round: The number of rounds to run the training.


# Alpha: L1 regularization term on weights. Increasing this value makes models more conservative.

# colsample_by_tree: fraction of features that will be used to train each tree.

# eta: Step size shrinkage used in updates to prevent overfitting. 
# After each boosting step, eta parameter shrinks the feature weights to make the boosting process more conservative.


Xgboost_regressor1 = sagemaker.estimator.Estimator(container,
                                       role, 
                                       train_instance_count = 1, 
                                       train_instance_type = 'ml.m5.2xlarge',
                                       output_path = output_location,
                                       sagemaker_session = sagemaker_session)

#We can tune the hyper-parameters to improve the performance of the model

Xgboost_regressor1.set_hyperparameters(max_depth = 10,
                           objective = 'reg:linear',
                           colsample_bytree = 0.3,
                           alpha = 10,
                           eta = 0.1,
                           num_round = 100
                           )

# Creating "train", "validation" channels to feed in the model
# Source: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html

train_input = sagemaker.session.s3_input(s3_data = s3_train_data, content_type='csv',s3_data_type = 'S3Prefix')
valid_input = sagemaker.session.s3_input(s3_data = s3_validation_data, content_type='csv',s3_data_type = 'S3Prefix')


data_channels = {'train': train_input,'validation': valid_input}


Xgboost_regressor1.fit(data_channels)

"""# TASK #11: DEPLOY MODEL TO PERFORM INFERENCE"""

# Deploy the model to perform inference 

Xgboost_regressor = Xgboost_regressor1.deploy(initial_instance_count = 1, instance_type = 'ml.m5.2xlarge')

'''
Content type over-rides the data that will be passed to the deployed model, since the deployed model expects data
in text/csv format, we specify this as content -type.

Serializer accepts a single argument, the input data, and returns a sequence of bytes in the specified content
type

Reference: https://sagemaker.readthedocs.io/en/stable/predictors.html
'''
from sagemaker.predictor import csv_serializer, json_deserializer

Xgboost_regressor.content_type = 'text/csv'
Xgboost_regressor.serializer = csv_serializer
Xgboost_regressor.deserializer = None

X_test.shape

# making prediction

predictions1 = Xgboost_regressor.predict(X_test[0:10000])

predictions2 = Xgboost_regressor.predict(X_test[10000:20000])

predictions3 = Xgboost_regressor.predict(X_test[20000:30000])

predictions4 = Xgboost_regressor.predict(X_test[30000:31618])

# custom code to convert the values in bytes format to array

def bytes_2_array(x):
    
    # makes entire prediction as string and splits based on ','
    l = str(x).split(',')
    
    # Since the first element contains unwanted characters like (b,',') we remove them
    l[0] = l[0][2:]
    #same-thing as above remove the unwanted last character (')
    l[-1] = l[-1][:-1]
    
    # iterating through the list of strings and converting them into float type
    for i in range(len(l)):
        l[i] = float(l[i])
        
    # converting the list into array
    l = np.array(l).astype('float32')
    
    # reshape one-dimensional array to two-dimensional array
    return l.reshape(-1,1)

predicted_values_1 = bytes_2_array(predictions1)

predicted_values_1.shape

predicted_values_2 = bytes_2_array(predictions2)
predicted_values_2.shape

predicted_values_3 = bytes_2_array(predictions3)
predicted_values_3.shape

predicted_values_4 = bytes_2_array(predictions4)
predicted_values_4.shape

predicted_values = np.concatenate((predicted_values_1, predicted_values_2, predicted_values_3, predicted_values_4))

predicted_values.shape

from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from math import sqrt
k = X_test.shape[1]
n = len(X_test)
RMSE = float(format(np.sqrt(mean_squared_error(y_test, predicted_values)),'.3f'))
MSE = mean_squared_error(y_test, predicted_values)
MAE = mean_absolute_error(y_test, predicted_values)
r2 = r2_score(y_test, predicted_values)
adj_r2 = 1-(1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE, '\nMSE =',MSE, '\nMAE =',MAE, '\nR2 =', r2, '\nAdjusted R2 =', adj_r2)

# Delete the end-point

Xgboost_regressor.delete_endpoint()

"""# EXCELLENT JOB! YOU SHOULD BE PROUD OF YOUR NEWLY ACQUIRED SKILLS"""